{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalRetrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilasha-kumar/modeling-lexical-retrieval/blob/main/LexicalRetrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkUSyf_WeMwt"
      },
      "source": [
        "# Allow drive access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bFV9y2TXkgz",
        "outputId": "d4ee8d7d-4dc6-4159-bda5-eb267c294599"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53eYB-A0eP98"
      },
      "source": [
        "# GPU access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2HxbC0YYEkR",
        "outputId": "d20c84e8-e758-4752-a48a-d2a67e60aedb"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Mon Nov 22 20:13:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    33W / 250W |    375MiB / 16280MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cmvK8BJ8SV5"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import heapq\n",
        "import itertools\n",
        "import scipy.spatial.distance\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from numpy.random import randint\n",
        "from scipy.special import softmax\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from numpy.linalg import matrix_power\n",
        "from functools import lru_cache\n",
        "import glob\n",
        "from scipy.special import expit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from functools import lru_cache\n",
        "from itertools import product as iterprod\n",
        "import itertools\n",
        "from nltk.metrics import *\n",
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgV7A0mY8v2s"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gycn2xZ08Ul2",
        "outputId": "3677be2d-a677-4832-f149-79a7a5647f61"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  julie_files = pd.read_csv(\"/content/drive/My Drive/LexicalRetrieval-2021/Julie_2021data.csv\", encoding= 'unicode_escape')\n",
        "  vocab = pd.read_csv(\"/content/drive/My Drive/LexicalRetrieval-2021/julie_vocab.csv\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (23) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "bBtAAkWcB25V",
        "outputId": "f5578d37-9d0c-44d1-f5f3-84518230d37f"
      },
      "source": [
        "julie_files.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>StudyNo</th>\n",
              "      <th>AgeGroup</th>\n",
              "      <th>PrimeInstruction</th>\n",
              "      <th>ExperimentName</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Procedure</th>\n",
              "      <th>Trial</th>\n",
              "      <th>Both</th>\n",
              "      <th>Neither</th>\n",
              "      <th>Phonological</th>\n",
              "      <th>Semantic</th>\n",
              "      <th>Prime</th>\n",
              "      <th>PrimeCondition</th>\n",
              "      <th>Question.RESP</th>\n",
              "      <th>State.RT</th>\n",
              "      <th>FreeResp</th>\n",
              "      <th>Resp</th>\n",
              "      <th>FreeResp.RT</th>\n",
              "      <th>NewAccuracy</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>FreeRespPrimeIntrusion</th>\n",
              "      <th>Target</th>\n",
              "      <th>MCCorrect</th>\n",
              "      <th>McResp</th>\n",
              "      <th>McAcc</th>\n",
              "      <th>McRT</th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "      <th>McActualResp</th>\n",
              "      <th>WhichPrime</th>\n",
              "      <th>ToEliminate</th>\n",
              "      <th>prompt</th>\n",
              "      <th>both_prompt</th>\n",
              "      <th>neither_prompt</th>\n",
              "      <th>sem_prompt</th>\n",
              "      <th>phon_prompt</th>\n",
              "      <th>prime_prompt</th>\n",
              "      <th>target_prompt</th>\n",
              "      <th>resp_prompt</th>\n",
              "      <th>target_both</th>\n",
              "      <th>target_neither</th>\n",
              "      <th>target_sem</th>\n",
              "      <th>target_phon</th>\n",
              "      <th>target_prime</th>\n",
              "      <th>target_resp</th>\n",
              "      <th>lev_target_both</th>\n",
              "      <th>lev_target_neither</th>\n",
              "      <th>lev_target_sem</th>\n",
              "      <th>lev_target_phon</th>\n",
              "      <th>lev_target_prime</th>\n",
              "      <th>lev_target_resp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>Old</td>\n",
              "      <td>NotThePrime</td>\n",
              "      <td>tot not the prime</td>\n",
              "      <td>701</td>\n",
              "      <td>ExpProc1</td>\n",
              "      <td>1</td>\n",
              "      <td>avoid</td>\n",
              "      <td>dove</td>\n",
              "      <td>absolve</td>\n",
              "      <td>refuse</td>\n",
              "      <td>ABSOLVE</td>\n",
              "      <td>P</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12345</td>\n",
              "      <td>1527</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>abstain</td>\n",
              "      <td>c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>refuse</td>\n",
              "      <td>dove</td>\n",
              "      <td>abstain</td>\n",
              "      <td>absolve</td>\n",
              "      <td>avoid</td>\n",
              "      <td>0</td>\n",
              "      <td>X</td>\n",
              "      <td>0</td>\n",
              "      <td>To refrain deliberately and often with an effo...</td>\n",
              "      <td>0.066920</td>\n",
              "      <td>-0.033121</td>\n",
              "      <td>0.171530</td>\n",
              "      <td>0.127612</td>\n",
              "      <td>0.127612</td>\n",
              "      <td>0.125816</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.312840</td>\n",
              "      <td>0.089485</td>\n",
              "      <td>0.464569</td>\n",
              "      <td>0.217932</td>\n",
              "      <td>0.217932</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Old</td>\n",
              "      <td>NotThePrime</td>\n",
              "      <td>tot not the prime</td>\n",
              "      <td>701</td>\n",
              "      <td>ExpProc1</td>\n",
              "      <td>2</td>\n",
              "      <td>Norderstedt</td>\n",
              "      <td>image</td>\n",
              "      <td>neurosurgery</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>NORDERSTEDT</td>\n",
              "      <td>B</td>\n",
              "      <td>2</td>\n",
              "      <td>6947</td>\n",
              "      <td>0</td>\n",
              "      <td>12345</td>\n",
              "      <td>1060</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Nuremberg</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>1</td>\n",
              "      <td>7385</td>\n",
              "      <td>Norderstedt</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>image</td>\n",
              "      <td>Nuremberg</td>\n",
              "      <td>neurosurgery</td>\n",
              "      <td>Nuremberg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>German city for which antisemitic laws were na...</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>-0.096533</td>\n",
              "      <td>0.164941</td>\n",
              "      <td>0.021730</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>0.225719</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.180054</td>\n",
              "      <td>0.007681</td>\n",
              "      <td>0.428896</td>\n",
              "      <td>0.328091</td>\n",
              "      <td>0.180054</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>Old</td>\n",
              "      <td>NotThePrime</td>\n",
              "      <td>tot not the prime</td>\n",
              "      <td>701</td>\n",
              "      <td>ExpProc1</td>\n",
              "      <td>3</td>\n",
              "      <td>hematoma</td>\n",
              "      <td>window</td>\n",
              "      <td>homeowner</td>\n",
              "      <td>contusion</td>\n",
              "      <td>HEMATOMA</td>\n",
              "      <td>B</td>\n",
              "      <td>4</td>\n",
              "      <td>5920</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12345</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>hemorrhage</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>1</td>\n",
              "      <td>5430</td>\n",
              "      <td>hematoma</td>\n",
              "      <td>contusion</td>\n",
              "      <td>window</td>\n",
              "      <td>hemorrhage</td>\n",
              "      <td>homeowner</td>\n",
              "      <td>hemorrhage</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>The escape of blood from vessels, including in...</td>\n",
              "      <td>0.155783</td>\n",
              "      <td>0.027197</td>\n",
              "      <td>0.066568</td>\n",
              "      <td>-0.063326</td>\n",
              "      <td>0.155783</td>\n",
              "      <td>0.326574</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.653506</td>\n",
              "      <td>0.210072</td>\n",
              "      <td>0.458587</td>\n",
              "      <td>0.151632</td>\n",
              "      <td>0.653506</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>Old</td>\n",
              "      <td>NotThePrime</td>\n",
              "      <td>tot not the prime</td>\n",
              "      <td>701</td>\n",
              "      <td>ExpProc1</td>\n",
              "      <td>4</td>\n",
              "      <td>Saigon</td>\n",
              "      <td>thigh</td>\n",
              "      <td>sofa</td>\n",
              "      <td>Tokyo</td>\n",
              "      <td>THIGH</td>\n",
              "      <td>U</td>\n",
              "      <td>1</td>\n",
              "      <td>4833</td>\n",
              "      <td>seoul</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>3272</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>b</td>\n",
              "      <td>b</td>\n",
              "      <td>1</td>\n",
              "      <td>4168</td>\n",
              "      <td>sofa</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Saigon</td>\n",
              "      <td>thigh</td>\n",
              "      <td>Tokyo</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Capital of South Korea</td>\n",
              "      <td>0.270083</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.378604</td>\n",
              "      <td>0.100953</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.609253</td>\n",
              "      <td>0.609253</td>\n",
              "      <td>0.424191</td>\n",
              "      <td>0.144735</td>\n",
              "      <td>0.464015</td>\n",
              "      <td>0.166060</td>\n",
              "      <td>0.144735</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>Old</td>\n",
              "      <td>NotThePrime</td>\n",
              "      <td>tot not the prime</td>\n",
              "      <td>701</td>\n",
              "      <td>ExpProc1</td>\n",
              "      <td>5</td>\n",
              "      <td>Heinola</td>\n",
              "      <td>shop</td>\n",
              "      <td>handkerchief</td>\n",
              "      <td>Oslo</td>\n",
              "      <td>OSLO</td>\n",
              "      <td>R</td>\n",
              "      <td>1</td>\n",
              "      <td>7553</td>\n",
              "      <td>helsinki</td>\n",
              "      <td>Helsinki</td>\n",
              "      <td>3497</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Helsinki</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>1</td>\n",
              "      <td>3366</td>\n",
              "      <td>Heinola</td>\n",
              "      <td>Oslo</td>\n",
              "      <td>shop</td>\n",
              "      <td>Helsinki</td>\n",
              "      <td>handkerchief</td>\n",
              "      <td>Helsinki</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Capital of Finland</td>\n",
              "      <td>0.034701</td>\n",
              "      <td>-0.015010</td>\n",
              "      <td>0.383043</td>\n",
              "      <td>-0.002959</td>\n",
              "      <td>0.383043</td>\n",
              "      <td>0.575294</td>\n",
              "      <td>0.575294</td>\n",
              "      <td>0.222391</td>\n",
              "      <td>0.110593</td>\n",
              "      <td>0.546045</td>\n",
              "      <td>0.080330</td>\n",
              "      <td>0.546045</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  ...  lev_target_prime  lev_target_resp\n",
              "0           0             0  ...          0.333333              NaN\n",
              "1           1             1  ...          0.142857              NaN\n",
              "2           2             2  ...          0.375000              NaN\n",
              "3           3             3  ...          0.000000              1.0\n",
              "4           4             4  ...          0.125000              1.0\n",
              "\n",
              "[5 rows x 57 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzBs_-3G85P2"
      },
      "source": [
        "# Import USE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y29lkvMWebIT"
      },
      "source": [
        "The Universal Sentence Encoder uses two different architctures to encode a string of any length into a compact high-dimensional vector representation -- the Deep Averaging Network (which is more of a bag-of-words approach) and the Transformer network (more predictive, attention-based). See link above for more details -- but DAN is generally faster and slightly less accurate than the Transformer model on NLP tasks (we might want to compare both). Below we see some examples of how we can use these \"vectors\" to find \"closest neighbors\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyMKr_vB8hck",
        "outputId": "175a02f0-cf7c-405a-e181-23bf9f83ef4d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "dan_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "#transformer_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\" \n",
        "model = hub.load(dan_url)\n",
        "print (\"module %s loaded\" % dan_url)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9DrYP8S61Qq",
        "outputId": "863143a3-814d-4072-d7df-7fd80c9c3020"
      },
      "source": [
        "model([\"Finland\"])[0].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0MFBFD1eaTl"
      },
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "nIa2gruXfg2F",
        "outputId": "5cf7ad22-e685-44ad-9d03-8e85b97bdc50"
      },
      "source": [
        "sent1 = \"Capital of Finland\"\n",
        "print(\"prompt is:\", sent1)\n",
        "sent1_vec = model([sent1])[0]\n",
        "print(\"sent1_vec is a numpy array of shape:\", sent1_vec.shape)\n",
        "resp = list(vocab.vocab_word)\n",
        "print(f\"our vocab has {len(resp)} words from which we will find the ones closest to our sentence...\")\n",
        "cosine_list = [cosine(sent1_vec, model([r])[0]) for r in resp]\n",
        "vocab[\"cosine\"] = cosine_list\n",
        "vocab.nlargest(10, \"cosine\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt is: Capital of Finland\n",
            "sent1_vec is a numpy array of shape: (512,)\n",
            "our vocab has 12619 words from which we will find the ones closest to our sentence...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-14dd9aa4394d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"our vocab has {len(resp)} words from which we will find the ones closest to our sentence...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcosine_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-14dd9aa4394d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"our vocab has {len(resp)} words from which we will find the ones closest to our sentence...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcosine_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    947\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "arphJYwdjOnK",
        "outputId": "12168eda-8100-4dd6-de71-2556ce856a9d"
      },
      "source": [
        "sent1 = \"Last name of author of Little Women\"\n",
        "print(\"prompt is:\", sent1)\n",
        "sent1_vec = model([sent1])[0]\n",
        "print(\"sent1_vec is a numpy array of shape:\", sent1_vec.shape)\n",
        "resp = list(vocab.vocab_word)\n",
        "print(f\"our vocab has {len(resp)} words from which we will find the ones closest to our sentence...\")\n",
        "cosine_list = [cosine(sent1_vec, model([r])[0]) for r in resp]\n",
        "vocab[\"cosine\"] = cosine_list\n",
        "vocab.nlargest(10, \"cosine\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt is: Last name of author of Little Women\n",
            "sent1_vec is a numpy array of shape: (512,)\n",
            "our vocab has 12619 words from which we will find the ones closest to our sentence...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vocab_word</th>\n",
              "      <th>cosine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Alcott</td>\n",
              "      <td>0.387014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>author</td>\n",
              "      <td>0.371520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11085</th>\n",
              "      <td>surname</td>\n",
              "      <td>0.349252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>Rowling</td>\n",
              "      <td>0.270690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8518</th>\n",
              "      <td>petite</td>\n",
              "      <td>0.231320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>runt</td>\n",
              "      <td>0.224465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7432</th>\n",
              "      <td>midget</td>\n",
              "      <td>0.220665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1944</th>\n",
              "      <td>biography</td>\n",
              "      <td>0.219154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12544</th>\n",
              "      <td>writer</td>\n",
              "      <td>0.217155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Austen</td>\n",
              "      <td>0.214192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      vocab_word    cosine\n",
              "41        Alcott  0.387014\n",
              "83        author  0.371520\n",
              "11085    surname  0.349252\n",
              "779      Rowling  0.270690\n",
              "8518      petite  0.231320\n",
              "780         runt  0.224465\n",
              "7432      midget  0.220665\n",
              "1944   biography  0.219154\n",
              "12544     writer  0.217155\n",
              "81        Austen  0.214192"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "Y5blPBwEj2Ti",
        "outputId": "29fcf27e-6994-40d6-c6f4-ba7afaf64dd4"
      },
      "source": [
        "sent1 = \"Instrument for performing calculations by sliding beads along rods or grooves\"\n",
        "print(\"prompt is:\", sent1)\n",
        "sent1_vec = model([sent1])[0]\n",
        "print(\"sent1_vec is a numpy array of shape:\", sent1_vec.shape)\n",
        "resp = list(vocab.vocab_word)\n",
        "print(f\"our vocab has {len(resp)} words from which we will find the ones closest to our sentence...\")\n",
        "cosine_list = [cosine(sent1_vec, model([r])[0]) for r in resp]\n",
        "vocab[\"cosine\"] = cosine_list\n",
        "vocab.nlargest(10, \"cosine\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt is: Instrument for performing calculations by sliding beads along rods or grooves\n",
            "sent1_vec is a numpy array of shape: (512,)\n",
            "our vocab has 12619 words from which we will find the ones closest to our sentence...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vocab_word</th>\n",
              "      <th>cosine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abacus</td>\n",
              "      <td>0.361816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>calculations</td>\n",
              "      <td>0.336181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2405</th>\n",
              "      <td>calculation</td>\n",
              "      <td>0.305555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6389</th>\n",
              "      <td>instruments</td>\n",
              "      <td>0.305101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>grooves</td>\n",
              "      <td>0.302247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12417</th>\n",
              "      <td>wind instrument</td>\n",
              "      <td>0.292259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>beads</td>\n",
              "      <td>0.290021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6388</th>\n",
              "      <td>instrument</td>\n",
              "      <td>0.284906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471</th>\n",
              "      <td>Instrument</td>\n",
              "      <td>0.284906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7695</th>\n",
              "      <td>musical instrument</td>\n",
              "      <td>0.276964</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               vocab_word    cosine\n",
              "2                  abacus  0.361816\n",
              "148          calculations  0.336181\n",
              "2405          calculation  0.305555\n",
              "6389          instruments  0.305101\n",
              "399               grooves  0.302247\n",
              "12417     wind instrument  0.292259\n",
              "104                 beads  0.290021\n",
              "6388           instrument  0.284906\n",
              "471            Instrument  0.284906\n",
              "7695   musical instrument  0.276964"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74M1_PAWB3w8"
      },
      "source": [
        "The Transformer based USE model is pretty accurate in and of itself, whereas the DAN is not so accurate. But we want to model a \"human\" version of this model, so we can add some stochastic noise to these estimates for both models eventually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlDMgQchCzZ9"
      },
      "source": [
        "# Create phoneme function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgMBYBZNjBIS"
      },
      "source": [
        "Here we create a function that takes any letter string and partitions it into phonemes based on arpabet. Then we compute a measure of \"normalized\" phonemic similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0phmKU8C2GW",
        "outputId": "03b2cf4c-b928-46b2-ff28-bf8de1344c8d"
      },
      "source": [
        "# algo to obtain phonemes for any given strng\n",
        "# obtained from: https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
        "try:\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')\n",
        "    arpabet = nltk.corpus.cmudict.dict()\n",
        "\n",
        "@lru_cache()\n",
        "def wordbreak(s):\n",
        "    s = s.lower()\n",
        "    if s in arpabet:\n",
        "        return arpabet[s]\n",
        "    middle = len(s)/2\n",
        "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
        "    for i in partition:\n",
        "        pre, suf = (s[:i], s[i:])\n",
        "        if pre in arpabet and wordbreak(suf) is not None:\n",
        "            return [x+y for x,y in iterprod(arpabet[pre], wordbreak(suf))]\n",
        "    return None\n",
        "\n",
        "def normalized_sim(w1, w2):\n",
        "  return 1-edit_distance(w1,w2)/(max(len(w1), len(w2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yep5HnUC7Qz",
        "outputId": "f953481e-64c0-4f55-aa23-22f9c035c91b"
      },
      "source": [
        "w1 = \"bird\"\n",
        "w2 = \"burden\"\n",
        "print(\"wordbreak(w1)[0]:\",wordbreak(w1)[0])\n",
        "print(\"wordbreak(w2)[0]:\",wordbreak(w2)[0])\n",
        "\n",
        "print(\"normalized orthographic similarity (letters):\", normalized_sim(w1, w2))\n",
        "print(\"normalized phonemic similarity:\", normalized_sim(wordbreak(w1)[0],wordbreak(w2)[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wordbreak(w1)[0]: ['B', 'ER1', 'D']\n",
            "wordbreak(w2)[0]: ['B', 'ER1', 'D', 'AH0', 'N']\n",
            "normalized orthographic similarity (letters): 0.5\n",
            "normalized phonemic similarity: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwBRDTKA7CPm"
      },
      "source": [
        "## semantic "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "725XJuMrhS0B"
      },
      "source": [
        "Below we get cosines from the prompt to the different primes and targets in our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob6NA8PgAZII",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "b93904f3-fb6b-48d3-fc8f-5bd5bc9e748f"
      },
      "source": [
        "## get cosines for:\n",
        "# 1. prompt - target\n",
        "# 2. prompt - primes\n",
        "# 3. prompt - resp\n",
        "\n",
        "both_prompt = []\n",
        "neither_prompt = []\n",
        "phon_prompt = []\n",
        "sem_prompt = []\n",
        "prime_prompt = []\n",
        "target_prompt = []\n",
        "resp_prompt = []\n",
        "\n",
        "# 4. target- primes\n",
        "target_both = []\n",
        "target_neither = []\n",
        "target_sem = []\n",
        "target_phon = []\n",
        "target_prime = []\n",
        "# 5. target - answer\n",
        "target_resp = []\n",
        "\n",
        "\n",
        "for index, row in julie_files.iterrows():\n",
        "  prompt_vec = model([row[\"prompt\"]])[0]\n",
        "  target_vec = model([row[\"Target\"]])[0]\n",
        "  resp = re.sub('[^a-zA-Z]+', '', str(row[\"Resp\"]))\n",
        "  prime = str(row[\"Prime\"])\n",
        "  #print(\"resp =\", resp)\n",
        "  \n",
        "\n",
        "  both_prompt_sim = cosine(prompt_vec, model([row[\"Both\"]])[0])\n",
        "  both_prompt.append(both_prompt_sim)\n",
        "\n",
        "  neither_prompt_sim = cosine(prompt_vec, model([row[\"Neither\"]])[0])\n",
        "  neither_prompt.append(neither_prompt_sim)\n",
        "\n",
        "  phon_prompt_sim = cosine(prompt_vec, model([row[\"Phonological\"]])[0])\n",
        "  phon_prompt.append(phon_prompt_sim)\n",
        "\n",
        "  sem_prompt_sim = cosine(prompt_vec, model([row[\"Semantic\"]])[0])\n",
        "  sem_prompt.append(sem_prompt_sim)\n",
        "\n",
        "  prime_prompt_sim = cosine(prompt_vec, model([prime])[0])\n",
        "  prime_prompt.append(prime_prompt_sim)\n",
        "\n",
        "  target_prompt_sim = cosine(prompt_vec, model([row[\"Target\"]])[0])\n",
        "  target_prompt.append(target_prompt_sim)\n",
        "  \n",
        "  resp_prompt_sim = cosine(prompt_vec, model([resp])[0]) if resp != \"\" else \"NA\"\n",
        "  resp_prompt.append(resp_prompt_sim)\n",
        "  \n",
        "\n",
        "  #4. target- primes\n",
        "  target_both_sim = cosine(target_vec, model([row[\"Both\"]])[0])\n",
        "  target_both.append(target_both_sim)\n",
        "\n",
        "  target_neither_sim = cosine(target_vec, model([row[\"Neither\"]])[0])\n",
        "  target_neither.append(target_neither_sim)\n",
        "\n",
        "  target_sem_sim = cosine(target_vec, model([row[\"Semantic\"]])[0])\n",
        "  target_sem.append(target_sem_sim)\n",
        "\n",
        "  target_phon_sim = cosine(target_vec, model([row[\"Phonological\"]])[0])\n",
        "  target_phon.append(target_phon_sim)\n",
        "\n",
        "  target_prime_sim = cosine(target_vec, model([prime])[0])\n",
        "  target_prime.append(target_prime_sim)\n",
        "\n",
        "  # 5. target - answer\n",
        "  target_resp_sim = cosine(target_vec, model([resp])[0]) if resp != \"\" else \"NA\"\n",
        "  target_resp.append(target_resp_sim)\n",
        "\n",
        "\n",
        "julie_files[\"both_prompt\"]  = both_prompt\n",
        "julie_files[\"neither_prompt\"]  = neither_prompt\n",
        "julie_files[\"sem_prompt\"]  = sem_prompt\n",
        "julie_files[\"phon_prompt\"]  = phon_prompt\n",
        "\n",
        "julie_files[\"prime_prompt\"]  = prime_prompt\n",
        "julie_files[\"target_prompt\"]  = target_prompt\n",
        "julie_files[\"resp_prompt\"]  = resp_prompt\n",
        "\n",
        "julie_files[\"target_both\"]  = target_both\n",
        "julie_files[\"target_neither\"]  = target_neither\n",
        "julie_files[\"target_sem\"]  = target_sem\n",
        "julie_files[\"target_phon\"]  = target_phon\n",
        "julie_files[\"target_prime\"]  = target_prime\n",
        "julie_files[\"target_resp\"]  = target_resp\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1cd1a7a2678c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;31m# 5. target - answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   \u001b[0mtarget_resp_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NA\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m   \u001b[0mtarget_resp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_resp_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqUNfel34uOF"
      },
      "source": [
        "## phon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkUPd0LEha6-"
      },
      "source": [
        "Below we get estimates of phonemic similarity from the prompt to the different primes and targets in our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa1phbpr4cA2"
      },
      "source": [
        "## get normalized phonemic similarities for:\n",
        "\n",
        "# 4. target- primes\n",
        "target_both = []\n",
        "target_neither = []\n",
        "target_sem = []\n",
        "target_phon = []\n",
        "target_prime = []\n",
        "# 5. target - answer\n",
        "target_resp = []\n",
        "\n",
        "\n",
        "for index, row in julie_files.iterrows():\n",
        "  \n",
        "  resp = re.sub('[^a-zA-Z]+', '', str(row[\"Resp\"]))\n",
        "  semantic = re.sub('[^a-zA-Z]+', '', str(row[\"Semantic\"]))\n",
        "  phono = re.sub('[^a-zA-Z]+', '', str(row[\"Phonological\"]))\n",
        "  neither = re.sub('[^a-zA-Z]+', '', str(row[\"Neither\"]))\n",
        "  both = re.sub('[^a-zA-Z]+', '', str(row[\"Both\"]))\n",
        "  \n",
        "\n",
        "  prime = re.sub('[^a-zA-Z]+', '', str(row[\"Prime\"]))\n",
        "  #print(\"resp =\", resp)\n",
        "  \n",
        "\n",
        "  #4. target- primes\n",
        "  target_both_sim = normalized_sim(wordbreak(row[\"Target\"])[0],wordbreak(both)[0])\n",
        "  target_both.append(target_both_sim)\n",
        "\n",
        "  target_neither_sim = normalized_sim(wordbreak(row[\"Target\"])[0],wordbreak(neither)[0])\n",
        "  target_neither.append(target_neither_sim)\n",
        "\n",
        "  target_sem_sim = normalized_sim(wordbreak(row[\"Target\"])[0],wordbreak(semantic)[0])\n",
        "  target_sem.append(target_sem_sim)\n",
        "\n",
        "  target_phon_sim = normalized_sim(wordbreak(row[\"Target\"])[0],wordbreak(phono)[0])\n",
        "  target_phon.append(target_phon_sim)\n",
        "\n",
        "  target_prime_sim = normalized_sim(wordbreak(row[\"Target\"])[0],wordbreak(prime)[0])\n",
        "  target_prime.append(target_prime_sim)\n",
        "\n",
        "  # 5. target - answer\n",
        "  target_resp_sim = normalized_sim(wordbreak(row[\"Target\"])[0],wordbreak(resp)[0]) if resp != \"\" else \"NA\"\n",
        "  target_resp.append(target_resp_sim)\n",
        "\n",
        "julie_files[\"lev_target_both\"]  = target_both\n",
        "julie_files[\"lev_target_neither\"]  = target_neither\n",
        "julie_files[\"lev_target_sem\"]  = target_sem\n",
        "julie_files[\"lev_target_phon\"]  = target_phon\n",
        "julie_files[\"lev_target_prime\"]  = target_prime\n",
        "julie_files[\"lev_target_resp\"]  = target_resp\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRpS5r589YRv"
      },
      "source": [
        "julie_files.to_csv(\"/content/drive/My Drive/LexicalRetrieval-2021/Julie_2021data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2rStY1ShjaX"
      },
      "source": [
        "# Modeling Ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_ZuyN4hhmKq"
      },
      "source": [
        "Some ways to think about how to \"model\" the task:\n",
        "\n",
        "\n",
        "1. After obtaining the representation for the prompt and all words in the vocabulary, start an \"activation\" process such that at t = 0, activation \"spreads\" from the prompt to all words in proportion to their similarity to the prompt\n",
        "2. At t=1, those words further spread activation to their neighbors.\n",
        "3. This could continue for \"t\" time steps technically, but we can also introduce a prime at some time step. This \"prime\" gets some extra boost of activation (+5 units, say), and then similarities are assessed as a combination of prompt and cue to ultimately produce the response. \n",
        "4. Maybe the ideal way to do this is an \"activation\" matrix of size vocab x 1 for both semantic and phonology and then we merge the two eventually?\n",
        "5. We may want to add in some stochastic noise to simulate partial knowledge in these models to see how that changes things\n",
        "6. So a general process model might be:\n",
        "*   activate_prompt_neighbors(noise) returns a 1-d array of similarities to every word in the vocab\n",
        "*   activate_prime_neighbors(noise) returns two N-by-1 arrays of similarities+activation corresponding to semantic and phonological similarities\n",
        "*   combine_semantic_phonological(method = \"additive | multiplicative\") returns a single N-by-1 array corresponding to combined sem-phon similarities after the promot and prime activations have been activated\n",
        "*   generate_predictions() returns a softmax of the activated matrix\n",
        "7. Ultimately, we want to make the code below efficient, and simulate about 100 participant runs with different levels of \"noise\" corresponding to levels of knowledge to obtain different model predictions\n",
        "8. Also, we may want to have a parameter that controls the weight to semantic vs. phonological information in the \"combine\" function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmvT7nTQAMD2"
      },
      "source": [
        "## preparing/reducing data size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yIQzCKOw-4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf4bd93-850e-4620-ba4b-5a67b95c0653"
      },
      "source": [
        "## preparing data\n",
        "julie_files[\"ActualPrime\"] = np.where(julie_files['PrimeCondition'] == 'P', julie_files['Phonological'], \n",
        "                                      np.where(julie_files['PrimeCondition'] == 'B', julie_files['Both'], \n",
        "                                               np.where(julie_files['PrimeCondition'] == 'R', julie_files['Semantic'],julie_files['Neither']))) \n",
        "julie_files['prompt'] = julie_files['prompt'].str.replace('\\t',' ')                                      \n",
        "julie_files['prompt'] = julie_files['prompt'].str.strip()\n",
        "print(f\"full dataset is {len(julie_files)} rows\")\n",
        "## for target accuracy we only need 100 (prompts) x 4 (primes)\n",
        "targetacc_data = julie_files[[\"ActualPrime\", \"PrimeCondition\", \"Target\", \"prompt\"]].drop_duplicates()\n",
        "print(f\"target accuracy data is {len(targetacc_data)} rows\")\n",
        "## for response accuracy we need the unique responses for each prompt-prime combination\n",
        "\n",
        "respacc_data = julie_files[[\"ActualPrime\", \"PrimeCondition\", \"Target\", \"Resp\", \"prompt\"]].drop_duplicates()\n",
        "print(f\"response accuracy data is {len(respacc_data)} rows\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full dataset is 17400 rows\n",
            "target accuracy data is 400 rows\n",
            "response accuracy data is 4377 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIW13RMx0mZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0bf224e7-3f70-40de-83a1-6eccf654c3ae"
      },
      "source": [
        "## we reshape the data so that we can run the functions at the \"prompt\" level\n",
        "respacc_data = respacc_data.sort_values(by=['prompt'])\n",
        "resp_wide = respacc_data.pivot(index = [\"prompt\", \"Target\", \"Resp\"], columns = [\"PrimeCondition\"], values = [\"ActualPrime\"])\n",
        "resp_wide = resp_wide.reset_index()\n",
        "resp_wide.columns = resp_wide.columns.map('|'.join).str.strip('|')\n",
        "resp_wide = resp_wide[resp_wide.Resp != '12345']\n",
        "resp_wide = resp_wide.reset_index()\n",
        "resp_wide"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>prompt</th>\n",
              "      <th>Target</th>\n",
              "      <th>Resp</th>\n",
              "      <th>ActualPrime|B</th>\n",
              "      <th>ActualPrime|P</th>\n",
              "      <th>ActualPrime|R</th>\n",
              "      <th>ActualPrime|U</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>20th century American poet whose trademark was...</td>\n",
              "      <td>Cummings</td>\n",
              "      <td>Browning</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Browning</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20th century American poet whose trademark was...</td>\n",
              "      <td>Cummings</td>\n",
              "      <td>Carrol</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cummerbund</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>20th century American poet whose trademark was...</td>\n",
              "      <td>Cummings</td>\n",
              "      <td>Cummings</td>\n",
              "      <td>Cunningham</td>\n",
              "      <td>cummerbund</td>\n",
              "      <td>Browning</td>\n",
              "      <td>point</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>20th century American poet whose trademark was...</td>\n",
              "      <td>Cummings</td>\n",
              "      <td>Cunnigham</td>\n",
              "      <td>Cunningham</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>20th century American poet whose trademark was...</td>\n",
              "      <td>Cummings</td>\n",
              "      <td>Dickinson</td>\n",
              "      <td>Cunningham</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2665</th>\n",
              "      <td>2765</td>\n",
              "      <td>Word made by changing the order of letters in ...</td>\n",
              "      <td>anagram</td>\n",
              "      <td>palindrome</td>\n",
              "      <td>acronym</td>\n",
              "      <td>analytic</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>2766</td>\n",
              "      <td>Word made by changing the order of letters in ...</td>\n",
              "      <td>anagram</td>\n",
              "      <td>plenumbra</td>\n",
              "      <td>acronym</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>2767</td>\n",
              "      <td>Word made by changing the order of letters in ...</td>\n",
              "      <td>anagram</td>\n",
              "      <td>puzzle</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>puzzle</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>2768</td>\n",
              "      <td>Word made by changing the order of letters in ...</td>\n",
              "      <td>anagram</td>\n",
              "      <td>synonym</td>\n",
              "      <td>acronym</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>2769</td>\n",
              "      <td>Word made by changing the order of letters in ...</td>\n",
              "      <td>anagram</td>\n",
              "      <td>syntax</td>\n",
              "      <td>acronym</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2670 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  ... ActualPrime|U\n",
              "0         1  ...           NaN\n",
              "1         2  ...           NaN\n",
              "2         3  ...         point\n",
              "3         4  ...           NaN\n",
              "4         5  ...           NaN\n",
              "...     ...  ...           ...\n",
              "2665   2765  ...           NaN\n",
              "2666   2766  ...           NaN\n",
              "2667   2767  ...           NaN\n",
              "2668   2768  ...           NaN\n",
              "2669   2769  ...           NaN\n",
              "\n",
              "[2670 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxafB-c6Rl_3"
      },
      "source": [
        "# Computing all vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkIIL8nNS-0e",
        "outputId": "b0c584af-3002-412a-9cce-35101efdfa76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "targetacc_data"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ActualPrime</th>\n",
              "      <th>PrimeCondition</th>\n",
              "      <th>Target</th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>absolve</td>\n",
              "      <td>P</td>\n",
              "      <td>abstain</td>\n",
              "      <td>To refrain deliberately and often with an effo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Norderstedt</td>\n",
              "      <td>B</td>\n",
              "      <td>Nuremberg</td>\n",
              "      <td>German city for which antisemitic laws were named</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hematoma</td>\n",
              "      <td>B</td>\n",
              "      <td>hemorrhage</td>\n",
              "      <td>The escape of blood from vessels, including in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>thigh</td>\n",
              "      <td>U</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Capital of South Korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oslo</td>\n",
              "      <td>R</td>\n",
              "      <td>Helsinki</td>\n",
              "      <td>Capital of Finland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>terse</td>\n",
              "      <td>B</td>\n",
              "      <td>taciturn</td>\n",
              "      <td>Saying little, reserved, uncommunicative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>bagel</td>\n",
              "      <td>U</td>\n",
              "      <td>chameleon</td>\n",
              "      <td>A small lizard with skin that changes color to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>Sardinia</td>\n",
              "      <td>B</td>\n",
              "      <td>Sicily</td>\n",
              "      <td>The largest Mediterranean island; the Italian ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>Shelley</td>\n",
              "      <td>B</td>\n",
              "      <td>Shaw</td>\n",
              "      <td>Last name of Irish author well known for Pygma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>view</td>\n",
              "      <td>U</td>\n",
              "      <td>anachronism</td>\n",
              "      <td>Something out of keeping with the time in whic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     ActualPrime  ...                                             prompt\n",
              "0        absolve  ...  To refrain deliberately and often with an effo...\n",
              "1    Norderstedt  ...  German city for which antisemitic laws were named\n",
              "2       hematoma  ...  The escape of blood from vessels, including in...\n",
              "3          thigh  ...                             Capital of South Korea\n",
              "4           Oslo  ...                                 Capital of Finland\n",
              "..           ...  ...                                                ...\n",
              "395        terse  ...           Saying little, reserved, uncommunicative\n",
              "396        bagel  ...  A small lizard with skin that changes color to...\n",
              "397     Sardinia  ...  The largest Mediterranean island; the Italian ...\n",
              "398      Shelley  ...  Last name of Irish author well known for Pygma...\n",
              "399         view  ...  Something out of keeping with the time in whic...\n",
              "\n",
              "[400 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymrXP-Z3AQli",
        "outputId": "7911d79d-71f0-4585-e224-0dcbe6984bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def vectors(data, vocab):\n",
        "  prompts = pd.Series(data[\"prompt\"].unique())\n",
        "  p_vecs = np.array([model([x])[0].numpy() for x in prompts])\n",
        "  print(p_vecs.shape)\n",
        "\n",
        "  p_vectors_df = pd.DataFrame(p_vecs).transpose()\n",
        "  print(p_vectors_df.shape)\n",
        "  p_vectors_df.columns = prompts\n",
        "  print(p_vectors_df.head())\n",
        "  p_vectors_df.to_csv(\"/content/drive/My Drive/LexicalRetrieval-2021/PromptVectors_New.csv\", index = False)\n",
        "\n",
        "  words = list(vocab.vocab_word)\n",
        "  w_vecs = np.array([model([x])[0].numpy() for x in words])\n",
        "  print(w_vecs.shape)\n",
        "  w_vectors_df = pd.DataFrame(w_vecs).transpose()\n",
        "\n",
        "  print(w_vectors_df.shape)\n",
        "  w_vectors_df.columns = words\n",
        "  print(w_vectors_df.head())\n",
        "  w_vectors_df.to_csv(\"/content/drive/My Drive/LexicalRetrieval-2021/VocabVectors_New.csv\", index = False)\n",
        "\n",
        "vectors(targetacc_data, vocab)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 512)\n",
            "(512, 100)\n",
            "   To refrain deliberately and often with an effort of self-denial from an action or practice  ...  Happening by a lucky chance or by accident rather than by design\n",
            "0                                           0.014262                                           ...                                          -0.002915               \n",
            "1                                           0.018321                                           ...                                          -0.052756               \n",
            "2                                           0.026393                                           ...                                          -0.002790               \n",
            "3                                          -0.029389                                           ...                                           0.019612               \n",
            "4                                           0.018353                                           ...                                           0.037026               \n",
            "\n",
            "[5 rows x 100 columns]\n",
            "(12619, 512)\n",
            "(512, 12619)\n",
            "          A         a    abacus  ...       zoo      zoom  zucchini\n",
            "0 -0.034784 -0.034784 -0.066512  ... -0.048157 -0.033304  0.002661\n",
            "1 -0.061211 -0.061211 -0.080752  ... -0.001644 -0.030125 -0.056137\n",
            "2  0.014873  0.014873 -0.002557  ...  0.065526 -0.006886  0.010760\n",
            "3  0.045649  0.045649  0.005688  ...  0.006332  0.052492  0.014026\n",
            "4 -0.062827 -0.062827 -0.052223  ... -0.067385 -0.049020 -0.026196\n",
            "\n",
            "[5 rows x 12619 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr8_qAO_Jb0h",
        "outputId": "856b01d6-129e-43e4-8c50-6a7856fde2a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prompt_vectors = pd.read_csv(\"/content/drive/My Drive/LexicalRetrieval-2021/PromptVectors_New.csv\").transpose().values\n",
        "print(prompt_vectors.shape)\n",
        "vocab_vectors = pd.read_csv(\"/content/drive/My Drive/LexicalRetrieval-2021/VocabVectors_New.csv\").transpose().values\n",
        "print(vocab_vectors.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 512)\n",
            "(12619, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0tlmEPKT8eR",
        "outputId": "c1b0c5b3-778a-421c-aed6-9ce3b45260d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab_vectors"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.03478369, -0.0612108 ,  0.01487272, ..., -0.08291931,\n",
              "         0.02704698,  0.02225077],\n",
              "       [-0.03478369, -0.0612108 ,  0.01487272, ..., -0.08291931,\n",
              "         0.02704698,  0.02225077],\n",
              "       [-0.06651215, -0.08075156, -0.00255653, ..., -0.05545882,\n",
              "        -0.04395873, -0.04783109],\n",
              "       ...,\n",
              "       [-0.04815683, -0.00164409,  0.06552604, ..., -0.01926666,\n",
              "        -0.03728981, -0.01452305],\n",
              "       [-0.03330355, -0.03012526, -0.00688593, ..., -0.06848427,\n",
              "         0.07342961, -0.05125343],\n",
              "       [ 0.00266085, -0.05613688,  0.01076016, ...,  0.01087722,\n",
              "         0.04447107, -0.07329643]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhankmjvFTet",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "f3ea3b8c-973b-4b37-9f77-4d3b6538bab8"
      },
      "source": [
        "#type(prompt_vectors.loc[prompt_vectors['prompt'] == 'Capital of Finland']['vector'])\n",
        "type(vocab_vectors['vector'][1])\n",
        "#type(model(['Finland'])[0].numpy())\n",
        "#vocab_vectors.dtypes\n",
        "vocab_vectors['vector'][1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[-3.47836874e-02 -6.12108037e-02  1.48727233e-02  4.56494465e-02\\n -6.28268346e-02  5.75991943e-02  4.54291366e-02 -4.71985200e-03\\n  7.21940324e-02 -1.42298508e-02 -3.86049487e-02 -7.84563925e-03\\n  1.11535955e-02  5.44652482e-03 -2.57644411e-02  6.73596263e-02\\n -4.62830253e-02  1.94416121e-02  1.93428714e-02 -1.38024269e-02\\n  5.73023520e-02 -1.69547312e-02  1.20984009e-02  3.29768024e-02\\n -2.72133034e-02  2.05458105e-02  5.64014576e-02 -5.90456873e-02\\n  6.94512948e-02 -5.28240055e-02 -4.89232540e-02  4.63303663e-02\\n  3.92288193e-02 -2.17689946e-02  1.20553393e-02  4.23156209e-02\\n  5.12196124e-02  6.92089573e-02 -6.93309586e-03 -3.40477787e-02\\n -2.98424494e-02  2.56863087e-02 -5.71632460e-02 -7.40868524e-02\\n  2.06592288e-02  7.14892894e-02  2.81842891e-02 -2.59266905e-02\\n -6.14356995e-02 -2.18019094e-02 -4.13651504e-02  7.36313164e-02\\n  8.45041871e-02  9.38365534e-02  3.98415998e-02  4.84482311e-02\\n  6.71457946e-02  4.81260270e-02 -2.04726718e-02 -2.52612755e-02\\n  8.22378322e-03 -3.01759085e-03 -5.30666001e-02 -6.71298802e-02\\n -4.23663370e-02  2.70030778e-02  2.86006499e-02 -4.24904265e-02\\n  2.86031794e-02  3.16842198e-02  3.48177813e-02  4.29351516e-02\\n -4.30292226e-02  5.91886528e-02  1.00301309e-02  1.35859437e-02\\n -2.71772668e-02 -9.03200638e-03 -9.39548202e-03  2.37772353e-02\\n  5.64958295e-03  7.44380653e-02 -8.35612975e-03 -2.27243593e-03\\n  4.04483527e-02  7.14090243e-02 -8.38288814e-02  5.67663014e-02\\n  6.05751760e-02  6.42019513e-05 -6.04793839e-02  2.80574653e-02\\n -2.42276434e-02 -2.40128692e-02  6.70206994e-02  2.90947314e-02\\n -1.26832975e-02 -6.08185912e-03  5.91598973e-02  7.54742697e-03\\n -5.95824048e-03 -7.86861777e-02  7.39068817e-03 -5.66415749e-02\\n  1.43638132e-02 -5.27086928e-02  5.23666441e-02  4.07163985e-02\\n -1.64618120e-02  5.82048073e-02  1.08160134e-02  9.70520731e-03\\n -8.07485252e-04  4.65558916e-02  3.17523628e-02 -2.48968303e-02\\n -2.16490738e-02 -4.23563458e-02  3.60419322e-03  1.54612050e-03\\n  1.00292265e-02  4.82280413e-03 -3.48911993e-02  2.94492058e-02\\n  2.48450432e-02  8.42998624e-02  5.25402799e-02 -4.10931036e-02\\n -2.93426644e-02  1.69041082e-02 -2.12584138e-02  2.57257093e-02\\n  5.86973093e-02  2.85681877e-02  4.25795726e-02 -2.85355840e-02\\n  5.11178710e-02 -5.18943705e-02 -7.22376490e-03  1.15540680e-02\\n  3.57408337e-02  7.66710266e-02  8.27035382e-02 -1.70536954e-02\\n  5.33812568e-02  3.93775618e-03  2.34207623e-02 -5.33392690e-02\\n  1.61777698e-02 -4.66552638e-02  1.13547193e-02  4.28644642e-02\\n -2.74296254e-02 -4.07379940e-02 -3.86208855e-03 -9.24122483e-02\\n -2.11753305e-02  8.04228634e-02 -6.49358258e-02 -3.17514762e-02\\n  3.69127258e-03 -8.03060830e-03  3.77399335e-03  4.02605459e-02\\n -2.97462400e-02  2.53520533e-02 -3.96231050e-03 -6.75113639e-03\\n  5.86132146e-02 -3.14995460e-02  4.71919104e-02  3.90018546e-03\\n -9.98369511e-03 -2.09460258e-02 -3.01515516e-02  4.59016040e-02\\n  8.01609382e-02  3.43567953e-02 -2.95782853e-02 -7.27901384e-02\\n  1.50069157e-02  1.86304227e-02 -4.59006196e-03  9.42539126e-02\\n -9.08078393e-04  2.13475507e-02  3.26543264e-02 -8.72553792e-03\\n  7.14577213e-02 -6.53574243e-02  4.07032669e-03  6.36607930e-02\\n  5.51879406e-02 -5.03443442e-02  2.86439024e-02 -9.38900188e-02\\n  2.07993835e-02 -3.72560024e-02  3.25951464e-02  2.29969714e-02\\n  2.64214142e-03  1.54307215e-02  5.51719293e-02 -6.12589065e-03\\n -1.56522952e-02  5.29624075e-02 -5.88723980e-02 -3.29316743e-02\\n -6.31147698e-02 -5.84692918e-02  5.55972755e-02 -3.12780626e-02\\n  6.73836842e-02  6.64580539e-02 -7.18597770e-02 -1.25176311e-02\\n -1.44856088e-02  2.12151799e-02  4.24740203e-02 -3.26020606e-02\\n  4.27161679e-02  3.56280990e-02  5.14654256e-02  3.78165655e-02\\n -5.88603376e-04 -7.63847902e-02 -5.00021838e-02 -3.58553268e-02\\n -1.64817721e-02 -4.77123149e-02  7.11886585e-03 -1.08533138e-02\\n -3.28748859e-02  9.86862853e-02 -6.90310821e-02  2.77758464e-02\\n  2.08705831e-02  2.50942726e-02 -8.25679004e-02 -4.19489034e-02\\n  1.27225672e-03  3.65928523e-02 -4.04299945e-02 -3.78947444e-02\\n -6.53979182e-02  1.74462274e-02 -5.09329000e-03 -4.31048051e-02\\n  8.50135833e-02 -5.29053248e-02  5.90351112e-02  4.90770340e-02\\n  6.10149056e-02  5.33929393e-02  9.73193794e-02  2.55296146e-03\\n -3.40659544e-02  8.43821168e-02 -9.31642111e-03  4.78768814e-03\\n -3.07521857e-02  9.30413529e-02  6.66169375e-02 -7.38092884e-02\\n -8.75217281e-03  2.48105731e-02 -2.57274415e-02 -7.75627196e-02\\n -3.61474641e-02 -3.31810191e-02  1.74941756e-02  4.80401888e-02\\n  2.47295778e-02  5.23664504e-02  4.33865674e-02 -3.69371288e-02\\n  1.74046587e-02  3.08225304e-02  1.92475226e-02  4.56571439e-03\\n  3.02013289e-02  2.28811763e-02  3.44999209e-02  4.02767546e-02\\n -4.40680981e-02  4.68554609e-02  6.01648577e-02 -3.77348326e-02\\n -6.44737184e-02 -5.88576915e-03  9.73630697e-03 -4.70183939e-02\\n -7.60001391e-02 -6.65462613e-02  3.01391520e-02  5.91019839e-02\\n -5.38560003e-02  6.27918262e-03  4.07683700e-02 -2.04649344e-02\\n -5.16983867e-02 -2.63454020e-02 -5.65176681e-02 -4.62283418e-02\\n -6.54088631e-02 -1.01926271e-03  6.07312918e-02 -2.08468735e-02\\n  6.37641475e-02 -1.75717641e-02  3.35029103e-02 -9.55108702e-02\\n  2.71232091e-02 -3.03175114e-02 -1.11385882e-02 -6.21991325e-03\\n  7.04637403e-03 -7.57376924e-02 -3.53493122e-03 -7.11584277e-03\\n -6.80961413e-03 -4.21320945e-02 -4.39419784e-02 -5.02149202e-02\\n  5.69042824e-02  5.56370206e-02  7.13610614e-04  6.43501356e-02\\n  2.27458049e-02  4.32589091e-02 -3.24218385e-02  1.59127228e-02\\n -3.58013548e-02 -1.80430263e-02 -2.12389566e-02 -5.03022112e-02\\n  2.25738473e-02  2.05422472e-02 -2.71854121e-02  1.04087470e-02\\n  2.88331006e-02  2.91320495e-03  1.12126283e-02 -4.09577489e-02\\n -6.56681955e-02 -7.84352329e-03 -1.03608137e-02  3.77900377e-02\\n  5.30884489e-02  3.01726721e-02  1.47813372e-02  1.08669717e-02\\n  7.23736221e-03  7.14194542e-03 -3.19088586e-02  1.25260977e-02\\n -8.70004296e-02 -5.39403185e-02  4.19174433e-02 -7.40580037e-02\\n  7.63314078e-03 -2.45716074e-04  6.82926700e-02 -3.08682229e-02\\n -1.59511045e-02  7.40643442e-02  7.93144554e-02  1.71639957e-02\\n -4.49599093e-03 -1.88082382e-02  5.25847562e-02  1.32432692e-02\\n -5.91501407e-02  2.98859291e-02 -3.98444273e-02 -1.88017581e-02\\n  6.43376708e-02  3.24296504e-02  4.14849119e-03 -8.92093778e-02\\n  3.45551856e-02 -6.94459006e-02  4.78265099e-02 -8.99532288e-02\\n  6.46385401e-02 -8.08023959e-02  4.30690460e-02 -5.09505868e-02\\n  7.61761796e-03  3.50056998e-02 -7.99399763e-02 -2.93111838e-02\\n  6.72940910e-02 -7.51347318e-02 -6.20600469e-02 -1.82933137e-02\\n  5.35289533e-02 -5.14870472e-02  1.21385613e-02 -8.43586121e-03\\n  6.80723488e-02 -7.14216940e-03 -6.89179152e-02 -1.10156264e-03\\n -5.12870103e-02 -2.82947812e-03 -4.43538791e-03  6.41169250e-02\\n  3.91536877e-02 -1.05098188e-02 -1.27419373e-02 -3.79878730e-02\\n -5.78956641e-02  2.48726234e-02 -6.22743964e-02 -4.48590964e-02\\n -7.01626623e-03  7.51741603e-02  2.69628540e-02 -5.10079078e-02\\n  2.43501998e-02 -5.48473522e-02  1.34001281e-02  3.07548512e-02\\n -7.60270283e-02  6.35104030e-02  1.58197992e-02  9.44363698e-02\\n  5.54999970e-02  6.99044392e-02  2.70444956e-02  8.16732831e-03\\n -5.34146167e-02 -4.58175912e-02 -1.16315484e-03  4.00296561e-02\\n  1.58367194e-02  5.18753454e-02  1.83382835e-02  8.87052193e-02\\n  3.56815606e-02 -5.86676523e-02  6.72983676e-02  1.72517151e-02\\n  4.83966284e-02 -4.17433865e-02 -2.73905247e-02  2.38908944e-03\\n  2.86113151e-04 -2.77700666e-02 -1.09688351e-02  1.05301263e-02\\n -4.35296744e-02 -5.36135100e-02 -5.40905669e-02  5.55638485e-02\\n  3.70542370e-02 -5.70354275e-02 -5.72829135e-03 -3.84864537e-03\\n  2.35608090e-02  6.51072860e-02 -1.09440358e-02  1.69498958e-02\\n  4.88091558e-02  3.94200720e-02 -1.59172155e-02  3.06187179e-02\\n  3.26159671e-02  1.89865362e-02  5.69887571e-02 -1.33291772e-02\\n  4.94642146e-02  6.42119646e-02  7.91755691e-02  1.68701671e-02\\n  3.29075791e-02  8.28656182e-03  4.67437804e-02  6.35405555e-02\\n -1.90017316e-02  2.06156988e-02  6.55013323e-02 -2.95002740e-02\\n -7.22471578e-03 -3.77014875e-02 -3.40087246e-03 -1.99851673e-02\\n  3.79433148e-02 -1.01970797e-02  5.92631521e-03 -7.96745718e-02\\n -2.90175267e-02 -2.82829348e-02  5.35735190e-02  6.35190532e-02\\n  3.43657620e-02 -6.59138337e-02  9.14003477e-02 -1.66340079e-02\\n  7.60530122e-03 -3.62485759e-02  1.57014038e-02 -9.74069387e-02\\n  6.41681775e-02  1.88531075e-02  1.60232410e-02  2.16203276e-02\\n -3.04666907e-02 -8.29193145e-02  2.70469785e-02  2.22507715e-02]'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GLm2F7xFkjF"
      },
      "source": [
        "## model functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2zZTc7w_MY6",
        "outputId": "591afb8c-8ccc-4c93-ad79-5e628e06ea60"
      },
      "source": [
        "def initial_activation(vocab_words):\n",
        "  # returns an array of initial activations, currently zero, but eventually replace by word frequency\n",
        "  x = np.zeros((len(vocab_words),1)).T\n",
        "  return x\n",
        "\n",
        "initial_activation(vocab).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 12619)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_NpIO8BF_v6"
      },
      "source": [
        "def activate_prompt_neighbors(activations, prompt, noise_level):\n",
        "  ## takes in a 1-d array of current activations\n",
        "  ## computes a vector representation of the prompt and returns a vector of similarities to each word in vocab + activations\n",
        "  ## with some noise added to each estimate\n",
        "  noise = np.random.normal(0, noise_level, 1)\n",
        "  prompt_vec = prompt_vectors.loc[prompt_vectors['prompt'] == prompt]['vector']\n",
        "  resp = vocab_vectors['vector'].tolist()\n",
        "  cosine_list = np.array([cosine(prompt_vec, r) for r in resp]) + activations # eventually add noise\n",
        "  return cosine_list\n",
        "\n",
        "#x = activate_prompt_neighbors(initial_activation(vocab), vocab, \"capital of Finland\", 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Eo-nYLGBrF"
      },
      "source": [
        "def activate_prime_neighbors(activations_sem, prime):\n",
        "  ## takes in a 1-d array of semantic activations and returns \"primed\" activations for both semantic and phonological\n",
        "  prime_vec = vocab_vectors.loc[vocab_vectors['word'] == prime]['vector']\n",
        "  resp = vocab_vectors['vector'].tolist()\n",
        "  semantic = (np.array([cosine(prime_vec, r) for r in resp]) + activations_sem)\n",
        "  phon = np.array([normalized_sim(r, prime) for r in vocab_vectors['word'].tolist()]).reshape(semantic.shape)\n",
        "  assert semantic.shape == phon.shape\n",
        "  return semantic, phon\n",
        "\n",
        "#y, z = activate_prime_neighbors(x, vocab, 'Oslo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLqfertJKXeH"
      },
      "source": [
        "def combine_semantic_phonological(semantic, phonological, method = \"multiply\", alpha=0): #TODO: add alpha arg for add only\n",
        "  if method == \"add\":\n",
        "    wtds = alpha * semantic\n",
        "    wtdp = (1-alpha)*phonological\n",
        "    comb = np.add(wtds, wtdp)\n",
        "  else:\n",
        "    comb = np.multiply(semantic, phonological)\n",
        "  \n",
        "  return softmax(comb)\n",
        "\n",
        "#final_activations = combine_semantic_phonological(y, z, \"add\", 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI53gxtIRWgf"
      },
      "source": [
        "def generate_predictions(activations, vocab_words, topn = 10):\n",
        "  ## takes in final activations and generates the top10 predictions\n",
        "  return [list(vocab_words.vocab_word)[i] for i in np.argpartition(-activations, topn).flatten().tolist()[:topn]]\n",
        "\n",
        "#generate_predictions(final_activations, vocab, topn = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS7RFXfSRzaE"
      },
      "source": [
        "def lexical_retrieval_model(prompt, prime, vocab_words, alpha):\n",
        "  ## brings all functions together\n",
        "  init = initial_activation(vocab_words)\n",
        "  x = activate_prompt_neighbors(init, vocab_words, prompt, 0.1)\n",
        "  y, z = activate_prime_neighbors(x, vocab, prime)\n",
        "  final_add = combine_semantic_phonological(y, z, \"add\", alpha)\n",
        "  final_mult = combine_semantic_phonological(y, z, \"multiply\")\n",
        "  preds_add = generate_predictions(final_add, vocab_words, topn = 10)\n",
        "  preds_mult = generate_predictions(final_mult, vocab_words, topn = 10)\n",
        "\n",
        "  return preds_add, preds_mult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOhSccTCCeGt"
      },
      "source": [
        "## to-do\n",
        "\n",
        "1. now we should use the \"wide\" dataset above and run activate_prompt_neigbors only ONCE for each prompt \n",
        "2. so dont use the lexical_retrieval_model function but instead use the base functions (activate_prompt_neighbors etc.) by looping over the above dataset in a way that the prompt activation function is called only once and then you calculate the differnet prime activations for that same prompt activation array\n",
        "3. Let's just do this for alpha = 0.5 for now to see initial results \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHIaihrFTsDQ"
      },
      "source": [
        "# Measure target accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7czHT5ZmHSC"
      },
      "source": [
        "# modify functions below for targetacc_data\n",
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtrhb3jwc7h9"
      },
      "source": [
        "add_success = []\n",
        "mult_success = []\n",
        "prompt_acts = {}\n",
        "prime_acts = {}\n",
        "def accuracy_single(t, vocab_words, n, alpha, data):\n",
        "  # adds 1 to corresponding list if the response is in the top n predictions, else 0\n",
        "\n",
        "  for i in range(4,8):\n",
        "    if pd.isna(data.iloc[t][i]):\n",
        "      continue\n",
        "    \n",
        "    if data[\"prompt\"][t] not in prompt_acts:\n",
        "      init = initial_activation(vocab_words)\n",
        "      prompt_neighbors = activate_prompt_neighbors(init, data[\"prompt\"][t], 0.1)\n",
        "      prompt_acts[data[\"prompt\"][t]] = prompt_neighbors\n",
        "    else:\n",
        "      prompt_neighbors = prompt_acts[data[\"prompt\"][t]]\n",
        "    \n",
        "    if data.iloc[t][i] not in prime_acts:\n",
        "      a, b = activate_prime_neighbors(prompt_neighbors, data.iloc[t][i])\n",
        "      prime_acts[data.iloc[t][i]] = (a, b)\n",
        "    else:\n",
        "      a, b = prime_acts[data.iloc[t][i]]\n",
        "      \n",
        "    final_add = combine_semantic_phonological(a, b, \"add\", alpha)\n",
        "    final_mult = combine_semantic_phonological(a, b, \"multiply\")\n",
        "    preds_add = generate_predictions(final_add, vocab_words, topn = 10)\n",
        "    preds_mult = generate_predictions(final_mult, vocab_words, topn = 10)\n",
        "    if data[\"Resp\"][t] in preds_add[:n]:\n",
        "      add_success.append(1)\n",
        "    else:\n",
        "      add_success.append(0)\n",
        "    if data[\"Resp\"][t] in preds_mult[:n]:\n",
        "      mult_success.append(1)\n",
        "    else:\n",
        "      mult_success.append(0)\n",
        "\n",
        "def accuracy_overall(vocab_words, n, alpha, data):\n",
        "  # returns the overall probability that the target will appear in the top n words predicted by the model\n",
        "  for t in range(len(data.index)):\n",
        "    accuracy_single(t, vocab_words, n, alpha, data)\n",
        "  acc_add = np.mean(add_success)\n",
        "  acc_mult = np.mean(mult_success)\n",
        "  print(\"Accuracy of additive model:\", acc_add)\n",
        "  print(\"Accuracy of multiplicative model:\", acc_mult)\n",
        "  return acc_add, acc_mult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn8TPa4mqGi-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "b76817b8-0804-45f3-ccd5-db1bd4afa8ec"
      },
      "source": [
        "accuracy_single(1, vocab, 5, 0.5, targetacc_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ed2adfa63fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetacc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-6257581d54e3>\u001b[0m in \u001b[0;36maccuracy_single\u001b[0;34m(trial, vocab_words, n, alpha, data)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#print(\"target:\", julie_files[\"Target\"][t])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#print(\"prime:\", julie_files[\"Prime\"][t])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexical_retrieval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ActualPrime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#The Prime column is all uppercase. Does that matter?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0madd_success\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3509cf119566>\u001b[0m in \u001b[0;36mlexical_retrieval_model\u001b[0;34m(prompt, prime, vocab_words, alpha)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m## brings all functions together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivate_prompt_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivate_prime_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mfinal_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_semantic_phonological\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-77ac0b2d7f41>\u001b[0m in \u001b[0;36mactivate_prompt_neighbors\u001b[0;34m(activations, vocab_words, prompt, noise_level)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprompt_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mcosine_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;31m# eventually add noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcosine_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-77ac0b2d7f41>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprompt_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mcosine_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;31m# eventually add noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcosine_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "579P_viMfhIJ"
      },
      "source": [
        "accuracy_overall(vocab, 10, 0.5, resp_wide)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dovHmQAiwoT6"
      },
      "source": [
        "# plot accuracy as a function of n or alpha for both add and mult \n",
        "#nvals = [i for i in range(5, 11)] #change upper bound depending on value of topn used in generate_predictions()\n",
        "#plt.plot(nvals, accuracy_overall(vocab, nvals)[0], 'r--', nvals, accuracy_overall(vocab, nvals)[1]) \n",
        "a = [x * 0.1 for x in range(11)]\n",
        "yAdd = []\n",
        "yMult = []\n",
        "for i in a:\n",
        "  acc = accuracy_overall(vocab, 10, i)\n",
        "  yAdd.append(acc[0])\n",
        "  yMult.append(acc[1])\n",
        "  print(\"Completed with alpha\", i)\n",
        "\n",
        "plt.plot(a, yAdd, 'r--', a, yMult, 'bs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOLXKbovY4U7"
      },
      "source": [
        "# model predictions: examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sskbdFgUTPPT",
        "outputId": "3d41f1e8-cee0-4869-fe05-8aec50ea818a"
      },
      "source": [
        "add, mult = lexical_retrieval_model(\"Capital of Finland\", \"Helsinki\", vocab)\n",
        "print(\"predictions for additive model:\", add)\n",
        "print(\"predictions for multiplicative model:\", mult)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predictions for additive model: ['Helsinki', 'Finland', 'Czechoslovakia', 'Oslo', 'Nordic', 'Sweden', 'Berlin', 'Holland', 'Iceland', 'Celtic']\n",
            "predictions for multiplicative model: ['Holland', 'Czechoslovakia', 'Berlin', 'Celtic', 'Helsinki', 'ski', 'Alpine', 'herring', 'Pilsen', 'clingy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4aWABnETWbk",
        "outputId": "93fa0ab7-aec8-45d1-f41f-93f86cb47676"
      },
      "source": [
        "add, mult = lexical_retrieval_model(\"A mathematical expression consisting of two terms\", \"bilateral\", vocab)\n",
        "print(\"predictions for additive model:\", add)\n",
        "print(\"predictions for multiplicative model:\", mult)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predictions for additive model: ['biannual', 'bilateral', 'integral', 'mathematical', 'bisexual', 'partial', 'paternal', 'interval', 'binomial', 'mutual']\n",
            "predictions for multiplicative model: ['bilateral', 'biannual', 'integral', 'binomial', 'paternal', 'interval', 'bisexual', 'partial', 'plural', 'mathematical']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiGUMx5ZWhvp",
        "outputId": "5bafc958-b52e-4ee5-c8bc-56d98b914039"
      },
      "source": [
        "add, mult = lexical_retrieval_model(\"Identical in form; coinciding exactly when superimposed\", \"cognizant\", vocab)\n",
        "print(\"predictions for additive model:\", add)\n",
        "print(\"predictions for multiplicative model:\", mult)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predictions for additive model: ['constant', 'cognition', 'cognizant', 'cognitive', 'confidant', 'confident', 'congruent', 'ignorant', 'coinciding', 'recognition']\n",
            "predictions for multiplicative model: ['cognition', 'cognizant', 'cognitive', 'confident', 'confidant', 'constant', 'recognition', 'ignorant', 'coinciding', 'congruent']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZGs7LTRXjFZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}